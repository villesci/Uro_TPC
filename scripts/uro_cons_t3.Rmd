---
title: "uro_cons_t3"
author: "Andrew Villeneuve"
date: "5/28/2020"
output: 
   word_document: default
---
```{r setup,include=F}
library(tidyr)
library(dplyr)
library(segmented)
library(ggplot2)
library(here)
library(AER)
library(DHARMa)
library(devtools) 
library(AICcmodavg)
library(lme4)
library(glmmTMB)



uro.con<-read.csv(here::here("data/uro.consumption2.csv"),header=T)
uro.con$no.consumed<-as.character(uro.con$no.consumed)
uro.con$no.consumed<-as.numeric(uro.con$no.consumed)

uro.con<-subset(uro.con,TPC.Label!=(6212))

#uro.con<-filter(uro.con,no.consumed > 0)
##why did I do this earlier??^ 

#this step below goes one step beyond just filtering out snails that ran out of food at each timepoint. It removes each snail them categorically from analysis at all time points, which is extreme for lower timepoints but appropriate for the last timepoint.

#uro.con<-subset(uro.con,TPC.Label!=(6833)&TPC.Label!=(6831)&TPC.Label!=(6711)&TPC.Label!=(6632)&TPC.Label!=(6631)&TPC.Label!=(6611)&TPC.Label!=(6433)&TPC.Label!=(6422)&TPC.Label!=(6412)&TPC.Label!=(6411)&TPC.Label!=(6233)&TPC.Label!=(6212)&TPC.Label!=(6212)&TPC.Label!=(6111)&TPC.Label!=(5831)&TPC.Label!=(5813)&TPC.Label!=(5732)&TPC.Label!=(5731)&TPC.Label!=(5723)&TPC.Label!=(5711)&TPC.Label!=(5622)&TPC.Label!=(5531)&TPC.Label!=(5333)&TPC.Label!=(5313)&TPC.Label!=(5312)&TPC.Label!=(5213)&TPC.Label!=(5121)&TPC.Label!=(5112)&TPC.Label!=(4522)&TPC.Label!=(4521)&TPC.Label!=(4433)&TPC.Label!=(4432)&TPC.Label!=(4231)&TPC.Label!=(3821)&TPC.Label!=(3712)&TPC.Label!=(3433)&TPC.Label!=(3432)&TPC.Label!=(3421)&TPC.Label!=(3413)&TPC.Label!=(2422)&TPC.Label!=(2421)&TPC.Label!=(6832))




##temeperature data


gbj<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/gbj.csv"),header=T)
gbj$rdate<-as.POSIXct(gbj$DateTimeStamp,tz="","%m/%d/%Y%H:%M")
gbj$oce<-"a"
wh<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/wh.csv"),header=T)
wh$rdate<-as.POSIXct(wh$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
wh$oce<-"a"
oy<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/oy.csv"),header=T)
oy$rdate<-as.POSIXct(oy$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
oy$oce<-"a"
bf<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/bf.csv"),header=T)
bf$rdate<-as.POSIXct(bf$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
bf$oce<-"a"
fb<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/fb.csv"),header=T)
fb$rdate<-as.POSIXct(fb$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
fb$oce<-"a"
gcsk<-read.csv(here::here("data/environmental_data/rawAtlantic/atl/gcsk.csv"),header=T)
gcsk$rdate<-as.POSIXct(gcsk$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
gcsk$oce<-"a"

nah1516<-read.csv(here::here("data/environmental_data/rawPacific/pac/nahcotta_2015_2016.csv"),header=T)#2015 through August, 2016 data after that
nah1516$rdate<-as.POSIXct(nah1516$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
nah1516$oce<-"p"
hmi2<-read.csv(here::here("data/environmental_data/rawPacific/pac/hmi2.csv"),header=T)
hmi2$rdate<-as.POSIXct(hmi2$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")#Indian island 2015
hmi2$oce<-"p"
to3<-read.csv(here::here("data/environmental_data/rawPacific/pac/to3.csv"),header=T)
to3$rdate<-as.POSIXct(to3$DateTimeStamp,tz="","%m/%d/%Y%H:%M")##stitched 2014 and 2015 (post Nove. 21 data together)
to3$oce<-"p"

#create objects of tempreatures during summer only 
s.gbj<-filter(gbj,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.wh<-filter(wh,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.oy<-filter(oy,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.bf<-filter(bf,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.fb<-filter(fb,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.to3<-filter(to3,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.hmi2<-filter(hmi2,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.nah1516<-filter(nah1516,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.gcsk<-filter(gcsk,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")

##Quartiles
q<-data.frame("site" = c("gbj","wh","oy","bf","fb","nah1516","hmi2","to3","gcsk"),"quantile"=NA,"decile"=NA, "max"=NA, "mean"=NA,"summer mean"=NA,"seasonlength10"=NA,"seasonlength16"=NA)
q[1,2]<-quantile(s.gbj$WTMP,0.75,type=1)
q[2,2]<-quantile(s.wh$WTMP,0.75,type=1)
q[3,2]<-quantile(s.oy$WTMP,0.75,type=1)
q[4,2]<-quantile(s.bf$WTMP,0.75,type=1)
q[5,2]<-quantile(s.fb$WTMP,0.75,type=1)
q[6,2]<-quantile(s.nah1516$WTMP,0.75,type=1)
q[7,2]<-quantile(s.hmi2$WTMP,0.75,type=1)
q[8,2]<-quantile(s.to3$WTMP,0.75,type=1)
q[9,2]<-quantile(s.gcsk$WTMP,0.75,type=1)

#upper 90th
q[1,3]<-quantile(s.gbj$WTMP,0.9,type=1)
q[2,3]<-quantile(s.wh$WTMP,0.9,type=1)
q[3,3]<-quantile(s.oy$WTMP,0.9,type=1)
q[4,3]<-quantile(s.bf$WTMP,0.9,type=1)
q[5,3]<-quantile(s.fb$WTMP,0.9,type=1)
q[6,3]<-quantile(s.nah1516$WTMP,0.9,type=1)
q[7,3]<-quantile(s.hmi2$WTMP,0.9,type=1)
q[8,3]<-quantile(s.to3$WTMP,0.9,type=1)
q[9,3]<-quantile(s.gcsk$WTMP,0.9,type=1)

#maximum temperature
q[1,4]<-s.gbj %>%  summarise(Value = max(WTMP))
q[2,4]<-s.wh  %>% summarise(Value = max(WTMP))
q[3,4]<-s.oy  %>% summarise(Value = max(WTMP))
q[4,4]<-s.bf  %>% summarise(Value = max(WTMP))
q[5,4]<-s.fb  %>% summarise(Value = max(WTMP))
q[6,4]<-s.nah1516 %>% summarise(Value = max(WTMP))
q[7,4]<-s.hmi2  %>% summarise(Value = max(WTMP))
q[8,4]<-s.to3  %>% summarise(Value = max(WTMP))
q[9,4]<-s.gcsk  %>% summarise(Value=max(WTMP))

#means
q[1,5]<-mean(gbj$WTMP)
q[2,5]<-mean(wh$WTMP)
q[3,5]<-mean(oy$WTMP)
q[4,5]<-mean(bf$WTMP)
q[5,5]<-mean(fb$WTMP)
q[6,5]<-mean(nah1516$WTMP)
q[7,5]<-mean(hmi2$WTMP)
q[8,5]<-mean(to3$WTMP)
q[9,5]<-mean(gcsk$WTMP)

#summer means
#means
q[1,6]<-mean(s.gbj$WTMP)
q[2,6]<-mean(s.wh$WTMP)
q[3,6]<-mean(s.oy$WTMP)
q[4,6]<-mean(s.bf$WTMP)
q[5,6]<-mean(s.fb$WTMP)
q[6,6]<-mean(s.nah1516$WTMP)
q[7,6]<-mean(s.hmi2$WTMP)
q[8,6]<-mean(s.to3$WTMP)
q[9,6]<-mean(s.gcsk$WTMP)

#season length

q[1,8]<-gbj%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[2,8]<-wh%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[3,8]<-oy%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[4,8]<-bf%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[5,8]<-fb%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[6,8]<-nah1516%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[7,8]<-hmi2%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[8,8]<-to3%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally
q[9,8]<-gcsk%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>16)%>%tally

q[1,7]<-gbj%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[2,7]<-wh%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[3,7]<-oy%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[4,7]<-bf%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[5,7]<-fb%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[6,7]<-nah1516%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[7,7]<-hmi2%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[8,7]<-to3%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally
q[9,7]<-gcsk%>%mutate(day=as.Date(rdate,format="%Y-%m-%d"))%>%group_by(day)%>%dplyr::summarise(daily_wtmp=mean(WTMP))%>%na.omit()%>%filter(daily_wtmp>10)%>%tally

pac2<-rbind(hmi2,nah1516)
atll<-rbind(gcsk,fb,bf,oy,wh,gbj)

temp<-rbind(pac2,atll) #temperature data for all

means<-data.frame(with(temp,tapply(WTMP,site,mean)))

#summer means

summer.temp<-filter(temp,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.mean<-data.frame(with(summer.temp,tapply(WTMP,site,mean)))

temp$date<-as.Date(temp$rdate)

temp$site<-factor(temp$site, levels=c("gbj","wh","oy","bf","fb","GCSK","nah15","hmi2"))
#"Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"

#egg laying mean
lay_sk<-mean((filter(gcsk,rdate>"2018-03-01 00:00:00" & rdate< "2018-03-30 00:00:00"))$WTMP)
lay_fb<-mean((filter(fb,rdate>"2018-03-01 00:00:00" & rdate< "2018-03-30 00:00:00"))$WTMP)
lay_bf<-mean((filter(bf,rdate>"2018-03-15 00:00:00" & rdate< "2018-04-15 00:00:00"))$WTMP)
lay_oy<-mean((filter(oy,rdate>"2018-05-01 00:00:00" & rdate< "2018-05-30 00:00:00"))$WTMP)
lay_wh<-mean((filter(wh,rdate>"2018-05-15 00:00:00" & rdate< "2018-06-15 00:00:00"))$WTMP)
lay_gb<-mean((filter(gbj,rdate>"2018-06-01 00:00:00" & rdate< "2018-06-30 00:00:00"))$WTMP)
lay_hm<-mean((filter(hmi2,rdate>"2018-04-15 00:00:00" & rdate< "2018-05-15 00:00:00"))$WTMP)#inat
lay_wp<-mean((filter(nah1516,rdate>"2018-04-15 00:00:00" & rdate< "2018-05-15 00:00:00"))$WTMP)#ruesink
#max
lay_sk2<-mean((filter(gcsk,rdate>"2018-03-01 00:00:00" & rdate< "2018-05-30 00:00:00"))$WTMP)
lay_fb2<-mean((filter(fb,rdate>"2018-03-01 00:00:00" & rdate< "2018-05-30 00:00:00"))$WTMP)
lay_bf2<-mean((filter(bf,rdate>"2018-03-15 00:00:00" & rdate< "2018-05-30 00:00:00"))$WTMP)
lay_oy2<-mean((filter(oy,rdate>"2018-05-01 00:00:00" & rdate< "2018-07-30 00:00:00"))$WTMP)
lay_wh2<-mean((filter(wh,rdate>"2018-07-01 00:00:00" & rdate< "2018-08-31 00:00:00"))$WTMP)
lay_gb2<-mean((filter(gbj,rdate>"2018-07-01 00:00:00" & rdate< "2018-08-31 00:00:00"))$WTMP)
lay_hm2<-mean((filter(hmi2,rdate>"2018-06-1 00:00:00" & rdate< "2018-07-15 00:00:00"))$WTMP)#inat
lay_wp2<-mean((filter(nah1516,rdate>"2018-06-1 00:00:00" & rdate< "2018-07-15 00:00:00"))$WTMP)#ruesink



```
# The Study

For this part of the study, we measured the consumption rate of juvenile Urosalpinx cinerea . We placed snails in tea strainers within two days of hatching, and grew them in tea strainers separated by population for 24 days. Nine replicates per population were distributed in six temperatures, with three groups of three subreplicates in each temperature/population treatment. Each snail was given five oysters at the beginning of the experiment, and over three different timepoints we checked strainers for number of oysters consumed and added new oysters. While the time between checking and adding oysters differed, time between checking for each snail was recorded. In some cases, all oysters were consumed, and in this case we noted this and removed the snails from analysis (see silenced code for this). Here, we will analyze only the rate of consumption at the third timepoint, as differences between populations at t1 and t2 were minimal. At time point 3, all snails had been in the experiment for 10-13 days and ranged from 12-15 days old. We stopped recording consumption rate after 10-13 days and simply added oysters to fuel their growth. 

The analysis of these growth curves requires two steps: 

1) the creation of models that describe the curved TPC shape using segmented and quadratic models and

2) the extraction of breakpoints (thermal optima = x breakpoint, maximal trait performance = y optima) for each population's TPC and modeling which environmental factors best describe any patterns in these optima across populations. I do this across two methods - segmented and quadratic regression. We want to see if both methods give approximately the same results, and based on model outputs decide which method we should use.

These two steps are presented, separately, in the Data Analysis section. The organization of these analyses thusly:

* Broken stick regression - I create, analyze, and plot segmented regressions 
  * Breakpoint analysis, broken stick - I extract the thermal optima (x brkpt) and maximal trait performance (y brkpt) of each population's segmented regression

* Quadratic model - I create, analyze, and plot quadratic regressions. 
  * Breakpoint analysis, quadratic - I extract the thermal optima (x brkpt) and maximal trait performance (y brkpt) of each population's quadratic regression

## Metadata

### TPC.Label 

Unique code for each indiviudal snail, corresponding to population, temperature treatment

### tank.replicate

Bin number (1-3),  sub-replication in each temperature treatment.

### Population

Source population of each snail

### Temp

One of six common garden temperatures. 24 days exposure in degrees C

### Date Hatch

Date of juvenile hatching

### Date TPC

Date of juvenile placement in TPC common garden experiment. Within two days of Date Hatch

### timepoint

We measured consumption three times in the course of the common garden experiment. This indicates at which point we recorded this. 

### date.feed

The date that we checked oysters for consumption and added new ones. Also the date of each timepoint.

### no.consumed

The number of oysters consumed, indicated by a clear drill hole in the shell of an oyster. Oysters in the process of being drilled were not counted.

### duration

The number of days since the last date.feed.

### rate

The rate of oysters consumed per day, calculated as (no.consumed)/(duration)

### all.consumed

Whether all oysters were consumed in the course of the timepoint duration, in other words, did the snails run out of food? y/n

### Environmental Data

We extracted temperature data from each site. With this data, we calculated different environmental predictors that might explain patterns in growth. 

* "Quantile" (°C)  
  *The average SST of the upper 75th percentile of summer months (06/01/2018 - 09/30/2018)
* "Decile" (°C)
  *The average SST of the upper 90th percentile of summer months (06/01/2018 - 09/30/2018)
* "max" 
  *The maximum SST value recorded during summer months (06/01/2018 - 09/30/2018)
* "mean"  (°C)
  *The mean SST of the site, calculated across the entire year
* "summer mean" (°C)
  *The mean SST of the site, calculated during the summer months (06/01/2018 - 09/30/2018)
* "seasonlength10/16" (days)
  *The number of days where the average daily temperature exceeded a threshold. Here, we use 16C, since this was our lowest temperature in experimentation and resulted in very slow growth (accelerates at 20C) - from Cheng et al. 2017. However, things become signicant when performed at 10C. We can also use 12.5C and get somewhat significant results, after the breakpoint for Oxygen consumption found in Schick's dissertation (Schick 1971) 

We also used latitude as a predictor (not shown in table below)


```{r,include=T,echo=F,warning=F}
#plot of site temperatures
ggplot(data=temp,aes(x=date,y=WTMP,color=site))+scale_color_viridis_d(name="Site",labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),option="D")+theme_classic()+geom_vline(xintercept = 152)+geom_vline(xintercept = 273)+ylab("SST (°C)")+xlab("")+scale_fill_discrete(name="Site",labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"))+scale_x_date(date_labels = "%b")+theme(text=element_text(family="sanserif",size=14))+stat_summary(geom="line",fun.y=mean,size=2)+geom_point(alpha=0.0002,color="black",fill="white")+guides(color=guide_legend(override.aes=list(fill=NA)))
  

q


```

# Data Exploration
As explained in the introduction, we checked for consumption rate three times during the 24 day experiment. Each time we checked, was coded as t1, t2, or t3. Below, we can see why we focus on t3 for this anlaysis over the other two timepoints. A wider range of consumption rates. As a result, I will filter out timepoints 1 and 2 and keep t3 observations only. This is because site differences only begin to become obvious after some time, and any difference in consumption rates will therefore also only appear as snails continue to grow.
```{r,echo=F,warning=F}
uro.con$rate<-log(uro.con$rate)
ggplot(uro.con,aes(x=Temp,y=rate,color=Population,linetype=Population))+geom_smooth(se=F,method="loess")+facet_wrap(timepoint~.)+theme_classic()+theme(text=element_text(family="arial",size=22))+scale_x_continuous(breaks=c(16,20,24,26,28,30))+
  scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+
  scale_linetype_manual(values=c(1,1,1,1,1,1,4,4),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+
  scale_size_manual(values=c(1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+geom_jitter(width=0.5)+xlab("Temperature (°C)")+ylab("Consumption rate (log(oysters/day))")


uro.con$timepoint<-as.factor(uro.con$timepoint)

summary(glm(no.consumed~(Population)*(timepoint)*Temp,uro.con,family=nbinom2,offset=log(duration)))
summary(glm(no.consumed~(Population)*(timepoint),uro.con,family=nbinom2,offset=log(duration)))
summary(glm(no.consumed~(Population)*Temp,uro.con,family=nbinom2,offset=log(duration)))

uro.con3<-filter(uro.con,timepoint=="3")
uro.con2<-filter(uro.con,timepoint=="2")
uro.con1<-filter(uro.con,timepoint=="1")


summary(glm(no.consumed~(Population)*(Temp),uro.con3,family=nbinom2,offset=log(duration)))
summary(glm(no.consumed~(Population)*(Temp),uro.con2,family=nbinom2,offset=log(duration)))
summary(glm(no.consumed~(Population)*(Temp),uro.con1,family=nbinom2,offset=log(duration)))


summary(aov(no.consumed~Population*Temp*timepoint, uro.con,offset=log(duration)))



summary(aov(no.consumed~Population*Temp*timepoint, uro.con,offset=log(duration)))



```

Below is basic data exploration. Are the number consumed and rate zero-inflated? Potentially.We will have to use a poisson model clearly. 
```{r,echo=F,warning=F}
hist(uro.con3$no.consumed)
#is this zero-inflated?

hist(uro.con3$rate)



```

## Residuals of a full model

Below are analyses of several types of models that should model consumption rate over temperature and population. THe whole point of this section is to test which model families and structures are best to use later on when we construct segmented regressions. I used a poisson error distribution and a log transformed offset of duration to the number consumed to get the rate.  I used the log of no.consumption partially because this is what Cheng et al. 2017 did.

I check each of these models to test for overdispersion (and further zero inflation models below). I can fix overdispersion, but the behavior of the residuals seem to be difficult to solve. 

```{r,waraning=F,echo=F}

full<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate), uro.con3, family = "poisson",offset=(log(duration)))

fullz <-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate), uro.con3, family = "poisson",offset=(log(duration)),ziformula=~1)

fullzOLRE<-glmmTMB(no.consumed ~ Population *Temp  +(1|tank.replicate)+(1|TPC.Label), uro.con3, family = "poisson",offset=(log(duration)),ziformula=~1)

binomzOLRE<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate)+(1|TPC.Label), uro.con3, family = "nbinom1",offset=(log(duration)),ziformula=~1)

binomz<-glmmTMB(no.consumed ~ Population *Temp  +(1|tank.replicate), uro.con3, family = "nbinom1",offset=(log(duration)),ziformula=~1)

binomzOLRE2<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate)+(1|TPC.Label), uro.con3, family = "nbinom2",offset=(log(duration)),ziformula=~1)

binomz2<-glmmTMB(no.consumed ~ Population *Temp  +(1|tank.replicate), uro.con3, family = "nbinom2",offset=(log(duration)),ziformula=~1)

fullOLRE<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate)+(1|TPC.Label), uro.con3, family = "poisson",offset=(log(duration)))

binomOLRE<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate)+(1|TPC.Label), uro.con3, family = "nbinom1",offset=(log(duration)))

binom<-glmmTMB(no.consumed ~ Population *Temp  +(1|tank.replicate), uro.con3, family = "nbinom1",offset=(log(duration)))

binomOLRE2<-glmmTMB(no.consumed ~ Population *Temp  + (1|tank.replicate)+(1|TPC.Label), uro.con3, family = "nbinom2",offset=(log(duration)))

binom2<-glmmTMB(no.consumed ~ Population *Temp  +(1|tank.replicate), uro.con3, family = "nbinom2",offset=(log(duration)))


bbmle::AICctab(full,fullz,fullzOLRE,binomzOLRE,binomz,binomzOLRE2,binomz2,fullOLRE,binomOLRE,binom,binomOLRE2,binom2)
#up in the air whether zero inflated helps.




sim_full<-simulateResiduals(fullzOLRE)
testDispersion(sim_full) #no overdispersion
plot(sim_full) # but look at those patterns in the quantiles of residuals v. predicted

testZeroInflation(sim_full)
#doesn't look zero inflated, but we need to directly compare

summary(binomz2)
```

A couple of observations: one, the log of duration as an offset to no.consumed appears to work well. Residuals are normal. However, still concerned about the zero inflation and a dispersion of 1.2, which is not signficantly above 1 but still of concern. DHARMa seems to confirm that this is not an issue.

Let's test for zero-inflation

```{r,echo=F,warning=F}
testZeroInflation(sim_full)
#doesn't look zero inflated, but we need to directly compare

bbmle::AICctab(full,fullz,fullzOLRE,binomzOLRE,binomz,binomzOLRE2,binomz2,fullOLRE,binomOLRE,binom,binomOLRE2,binom2)
#up in the air whether zero inflated helps.  

```



# Create Broken Stick Regressions

All of those models are a mess. Really,I just need to have segmented create broken stick models. Here, based on the information above, I chose a nbinom2 family with a log offset for duration.


It's hard to compare TPC against one another. One method we've settled on is the use of broken stick regression to allow us to quantify the shape of the reaction as well as the thermal optima (x) and the maximal trait performance (y). Here, we used the segmented package to create single-optima broken stick regressions that also allow us to extract optimas.

We used a separate model for each population, as segmented does not allow for grouping. We used the following formulation: glm(number consumed ~ temperature, population,family=nbinom2, offset=log(duration)), such that we modeled the response of consumption rate (number consumed offset by days) from a single population to temperature. 

```{r,warning=F,echo=F}
bf<-filter(uro.con3,Population=="Beaufort")
fb<-filter(uro.con3,Population=="Folly Beach")
gb<-filter(uro.con3,Population=="Great Bay")
hm<-filter(uro.con3,Population=="Humboldt")
oy<-filter(uro.con3,Population=="Oyster")
sk<-filter(uro.con3,Population=="Skidaway")
wp<-filter(uro.con3,Population=="Willapa")
wh<-filter(uro.con3,Population=="Woods Hole")


m.wh<-glm(no.consumed~Temp,wh,family='nbinom2',offset=log(duration))
seg.wh<-segmented(m.wh,seg.Z = ~Temp, psi=29)

m.gb<-glm(no.consumed~Temp,gb,family=nbinom2,offset=log(duration))
seg.gb<-segmented(m.gb,seg.Z = ~Temp, psi=24)

m.oy<-glm(no.consumed~Temp,oy,family=nbinom2,offset=log(duration))
seg.oy<-segmented(m.oy,seg.Z = ~Temp, psi=24)

m.bf<-glm(no.consumed~Temp,bf,family=nbinom2,offset=log(duration))
seg.bf<-segmented(m.bf,seg.Z = ~Temp, psi=29,seg.control(maxit.glm=15),fix.npsi=T,n.boot=50)

m.fb<-glm(no.consumed~Temp,fb,family=nbinom2,offset=log(duration))
seg.fb<-segmented(m.fb,seg.Z = ~Temp, psi=24)

m.sk<-glm(no.consumed~Temp,sk,family=nbinom2,offset=log(duration))
seg.sk<-segmented(m.sk,seg.Z = ~Temp, psi=24)

m.hm<-glm(no.consumed~Temp,hm,family=nbinom2,offset=log(duration))
seg.hm<-segmented(m.hm,seg.Z = ~Temp, psi=24)

m.wp<-glm(no.consumed~Temp,wp,family=nbinom2,offset=log(duration))
seg.wp<-segmented(m.wp,seg.Z = ~Temp, psi=24)

xmin<-min(uro.con3$Temp,na.rm=T)
xmax<-max(uro.con3$Temp,na.rm=T)

all.length.seg<-list(seg.gb,seg.wh,seg.oy,seg.bf,seg.fb,seg.sk,seg.wp,seg.hm)


##plotting predicted values
predicted.gb<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.gb$log_rate<-predict(seg.gb,predicted.gb)
predicted.gb$pop<-"gb"
predicted.gb$oce<-"a"

predicted.wh<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.wh$log_rate<-predict(seg.wh,predicted.wh)
predicted.wh$pop<-"wh"
predicted.wh$oce<-"a"

predicted.oy<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.oy$log_rate<-predict(seg.oy,predicted.oy)
predicted.oy$pop<-"oy"
predicted.oy$oce<-"a"

predicted.bf<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.bf$log_rate<-predict(seg.bf,predicted.bf)
predicted.bf$pop<-"bf"
predicted.bf$oce<-"a"

predicted.fb<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.fb$log_rate<-predict(seg.fb,predicted.fb)
predicted.fb$pop<-"fb"
predicted.fb$oce<-"a"

predicted.sk<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.sk$log_rate<-predict(seg.sk,predicted.sk)
predicted.sk$pop<-"sk"
predicted.sk$oce<-"a"

predicted.wp<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.wp$log_rate<-predict(seg.wp,predicted.wp)
predicted.wp$pop<-"wp"
predicted.wp$oce<-"p"

predicted.hm<-data.frame(Temp=seq(xmin,xmax,length.out=100))
predicted.hm$log_rate<-predict(seg.hm,predicted.hm)
predicted.hm$pop<-"hm"
predicted.hm$oce<-"p"

all.cons.pred<-rbind(predicted.gb,predicted.wh,predicted.oy,predicted.bf,predicted.fb,predicted.sk,predicted.wp,predicted.hm)
all.cons.pred$pop<-factor(all.cons.pred$pop,levels=c("gb","wh","oy","bf","fb","sk","wp","hm"))
all.cons.pred$Population<-ifelse(all.cons.pred$pop=="gb","Great Bay",ifelse(all.cons.pred$pop=="wh","Woods Hole",ifelse(all.cons.pred$pop=="oy","Oyster",ifelse(all.cons.pred$pop=="bf","Beaufort",ifelse(all.cons.pred$pop=="fb","Folly Beach",ifelse(all.cons.pred$pop=="sk","Skidaway",ifelse(all.cons.pred$pop=="wp","Willapa",ifelse(all.cons.pred$pop=="hm","Humboldt",NA))))))))
all.cons.pred$Population<-as.factor(all.cons.pred$Population)
all.cons.pred$Population<-factor(all.cons.pred$Population,levels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"))


uro.con$lograte<-log(uro.con$rate)
uro.con$Population<-factor(uro.con$Population,levels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"))


ggplot(all.cons.pred,aes(x=Temp,y=log_rate,group=pop))+geom_line(aes(group=pop,color=pop,linetype=pop),size=1.5)+
  ylab("Consumption rate (log(oysters/day))")+xlab("Temperature (°C)")+theme_classic()+
  scale_linetype_manual(values=c(1,1,1,1,1,1,4,4),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
  scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+scale_x_continuous(breaks=c(16,20,24,26,28,30))+theme(text=element_text(family="arial",size=22))

uro.con$rate<-log(uro.con$rate)

ggplot(all.cons.pred,aes(x=Temp,y=log_rate,group=pop))+geom_line(aes(group=pop,color=pop,linetype=pop),size=1.5)+
   ylab("Consumption rate (log(oysters/day))")+xlab("Common Garden Temperature")+theme_classic()+
   scale_linetype_manual(values=c(1,1,1,1,1,1,4,4),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
   scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+scale_x_continuous(breaks=c(16,20,24,26,28,30))+
   geom_point(data=uro.con,aes(x=Temp,y=rate,group=Population))+facet_wrap(Population~.)+theme(legend.position = "none") +theme(text=element_text(family="arial",size=22))


#for figure
consTPC<-ggplot(all.cons.pred,aes(x=Temp,y=log_rate,group=pop))+geom_line(aes(group=pop,color=pop,linetype=pop),size=1.5)+
  ylab("Consumption rate (log(oysters/day))")+xlab("Common Garden Temperature")+theme_classic()+
  scale_linetype_manual(values=c(1,1,1,1,1,1,4,4),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly B
                                                           each, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
  scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+scale_x_continuous(breaks=c(16,20,24,26,28,30))+theme(text=element_text(family="arial",size=22))


```


# Breakpoints extraction (silenced)

```{r,echo=F,warning=F}

 
   brkpts_con<-data.frame(matrix(,nrow=8,ncol=15))
   colnames(brkpts_con)<-c("pop","brkptx","brkpty","lat","mean","s.mean","q.mean","t.mean","oce","blank","brkptxq","brkptyq","max","seasonlength10","seasonlength16")
   
   brkpts_con$pop<-list("Willapa","Humboldt","Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway")
   
   brkpts_con[1,2]<-seg.wp$psi[[2]]
   brkpts_con[2,2]<-seg.hm$psi[[2]]
   brkpts_con[3,2]<-seg.gb$psi[[2]]
   brkpts_con[4,2]<-seg.wh$psi[[2]]
   brkpts_con[5,2]<-seg.oy$psi[[2]]
   brkpts_con[6,2]<-seg.bf$psi[[2]]
   brkpts_con[7,2]<-seg.fb$psi[[2]]
   brkpts_con[8,2]<-seg.sk$psi[[2]]
   
   brkpts_con[1,3]<-(seg.wp$psi[[2]]*coef(seg.wp)[[2]])+(coef(seg.wp)[[1]])
   brkpts_con[2,3]<-(seg.hm$psi[[2]]*coef(seg.hm)[[2]])+(coef(seg.hm)[[1]])
   brkpts_con[3,3]<-(seg.gb$psi[[2]]*coef(seg.gb)[[2]])+(coef(seg.gb)[[1]])
   brkpts_con[4,3]<-(seg.wh$psi[[2]]*coef(seg.wh)[[2]])+(coef(seg.wh)[[1]])
   brkpts_con[5,3]<-(seg.oy$psi[[2]]*coef(seg.oy)[[2]])+(coef(seg.oy)[[1]])
   brkpts_con[6,3]<-(seg.bf$psi[[2]]*coef(seg.bf)[[2]])+(coef(seg.bf)[[1]])
   brkpts_con[7,3]<-(seg.fb$psi[[2]]*coef(seg.fb)[[2]])+(coef(seg.fb)[[1]])
   brkpts_con[8,3]<-(seg.sk$psi[[2]]*coef(seg.sk)[[2]])+(coef(seg.sk)[[1]])
   
   #lat
   brkpts_con$lat<-ifelse(brkpts_con$pop=="Beaufort",34.819,ifelse(brkpts_con$pop=="Folly Beach",32.660525,
                                                           ifelse(brkpts_con$pop=="Great Bay",43.089589,ifelse(brkpts_con$pop=="Humboldt",40.849448,ifelse(brkpts_con$pop=="Oyster",
                                                                                                                                                37.288562,ifelse(brkpts_con$pop=="Tomales",38.12805,ifelse(brkpts_con$pop=="Woods Hole",41.57687,ifelse(brkpts_con$pop=="Willapa",46.5007,ifelse(brkpts_con$pop=="Skidaway",31.970
                                                                                                                                                                                                                                                                                        ,NA)))))))))
   #means
   brkpts_con[1,5]<-means[6,1]#wp
   brkpts_con[2,5]<-means[5,1]#hm
   brkpts_con[3,5]<-means[3,1]#gb
   brkpts_con[4,5]<-means[8,1]#wh
   brkpts_con[5,5]<-means[7,1]#oy
   brkpts_con[6,5]<-means[1,1]#bf
   brkpts_con[7,5]<-means[2,1]#fb
   brkpts_con[8,5]<-means[4,1]#sk
   
   #s.mean
   brkpts_con[1,6]<-s.mean[6,1]
   brkpts_con[2,6]<-s.mean[5,1]
   brkpts_con[3,6]<-s.mean[3,1]
   brkpts_con[4,6]<-s.mean[8,1]
   brkpts_con[5,6]<-s.mean[7,1]
   brkpts_con[6,6]<-s.mean[1,1]
   brkpts_con[7,6]<-s.mean[2,1]
   brkpts_con[8,6]<-s.mean[4,1]
   #q.means.
   brkpts_con[1,7]<-q[6,2]
   brkpts_con[2,7]<-q[7,2]
   brkpts_con[3,7]<-q[1,2]
   brkpts_con[4,7]<-q[2,2]
   brkpts_con[5,7]<-q[3,2]
   brkpts_con[6,7]<-q[4,2]
   brkpts_con[7,7]<-q[5,2]
   brkpts_con[8,7]<-quantile(s.gcsk$WTMP,0.75,type=1)
   #t.mean
   brkpts_con[1,8]<-q[6,3]
   brkpts_con[2,8]<-q[7,3]
   brkpts_con[3,8]<-q[1,3]
   brkpts_con[4,8]<-q[2,3]
   brkpts_con[5,8]<-q[3,3]
   brkpts_con[6,8]<-q[4,3]
   brkpts_con[7,8]<-q[5,3]
   brkpts_con[8,8]<-quantile(s.gcsk$WTMP,0.9,type=1)
   #max
   brkpts_con[1,13]<-q[6,4]
   brkpts_con[2,13]<-q[7,4]
   brkpts_con[3,13]<-q[1,4]
   brkpts_con[4,13]<-q[2,4]
   brkpts_con[5,13]<-q[3,4]
   brkpts_con[6,13]<-q[4,4]
   brkpts_con[7,13]<-q[5,4]
   brkpts_con[8,13]<-q[9,4]
   
   
#seasonlength10
brkpts_con[1,14]<-q[6,7]
brkpts_con[2,14]<-q[7,7]
brkpts_con[3,14]<-q[1,7]
brkpts_con[4,14]<-q[2,7]
brkpts_con[5,14]<-q[3,7]
brkpts_con[6,14]<-q[4,7]
brkpts_con[7,14]<-q[5,7]
brkpts_con[8,14]<-q[9,7]

#seasonlength16
brkpts_con[1,15]<-q[6,8]
brkpts_con[2,15]<-q[7,8]
brkpts_con[3,15]<-q[1,8]
brkpts_con[4,15]<-q[2,8]
brkpts_con[5,15]<-q[3,8]
brkpts_con[6,15]<-q[4,8]
brkpts_con[7,15]<-q[5,8]
brkpts_con[8,15]<-q[9,8]

   
   brkpts_con$oce<-ifelse(brkpts_con$pop=="Beaufort","a",ifelse(brkpts_con$pop=="Folly Beach","a",ifelse(brkpts_con$pop=="Great Bay","a",ifelse(brkpts_con$pop=="Humboldt","p",ifelse(brkpts_con$pop=="Oyster","a",ifelse(brkpts_con$pop=="Tomales","p",ifelse(brkpts_con$pop=="Woods Hole","a",ifelse(brkpts_con$pop=="Willapa","p",ifelse(brkpts_con$pop=="Skidaway","a",NA)))))))))
   
   brkpts_con$pop<-as.character(brkpts_con$pop)
   
   brkpts_con$spring<-NA
brkpts_con[8,16]<-lay_sk
brkpts_con[7,16]<-lay_fb
brkpts_con[6,16]<-lay_bf
brkpts_con[5,16]<-lay_oy
brkpts_con[4,16]<-lay_wh
brkpts_con[3,16]<-lay_gb
brkpts_con[2,16]<-lay_hm
brkpts_con[1,16]<-lay_wp

   brkpts_con$spring2<-NA
brkpts_con[8,17]<-lay_sk2
brkpts_con[7,17]<-lay_fb2
brkpts_con[6,17]<-lay_bf2
brkpts_con[5,17]<-lay_oy2
brkpts_con[4,17]<-lay_wh2
brkpts_con[3,17]<-lay_gb2
brkpts_con[2,17]<-lay_hm2
brkpts_con[1,17]<-lay_wp2
```

Here, we see that the stacking of regressions is not as clear as with shell length or weight (separate markdown). To see if countergradient variation in still present, we will have to extract breakpoints. First off, how confident are we in our breakpoints?

*P-score: The P-score tests the null hypothesis for no difference in slopes, i.e. no breakpoint. If P is below 0.05, then there is a breakpoint. We see here that not all P-scores are signficant, and therefore we fail to reject the null hypothesis of no change in slope, ie there are no breakpoints
 
*  CI Low/High: Confidence interval of the breakpoint
 
* Breakpoint: Breakpoint (x axis, thermal optima)
 
 
```{r,echo=F,warning=F}
scorescons<-data.frame("site" = c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),"breakpoint"=NA,"P Score"=NA,"Davies Test"=NA,"slope1"=NA,"CI slope 1"=NA,"slope2"=NA,"CI slope 2"=NA)
#brkpts x
scorescons[1,2]<-confint(seg.gb)[[1]]
scorescons[2,2]<-confint(seg.wh)[[1]]
scorescons[3,2]<-confint(seg.oy)[[1]]
scorescons[4,2]<-confint(seg.bf)[[1]]
scorescons[5,2]<-confint(seg.fb)[[1]]
scorescons[6,2]<-confint(seg.sk)[[1]]
scorescons[7,2]<-confint(seg.wp)[[1]]
scorescons[8,2]<-confint(seg.hm)[[1]]
#pscore
#scorescons[1,3]<-pscore.test(seg.gb,~temp)[[5]]
#scorescons[2,3]<-pscore.test(seg.wh,~temp)[[5]]
#scorescons[3,3]<-pscore.test(seg.oy,~temp)[[5]]
#scorescons[4,3]<-pscore.test(seg.bf,~temp)[[5]]
#scorescons[5,3]<-pscore.test(seg.fb,~temp)[[5]]
#scorescons[6,3]<-pscore.test(seg.sk,~temp)[[5]]
#scorescons[7,3]<-pscore.test(seg.wp,~temp)[[5]]
#scorescons[8,3]<-pscore.test(seg.hm,~temp)[[5]]
#davies
#scorescons[1,4]<-davies.test(seg.gb,~temp)[[5]]
#scorescons[2,4]<-davies.test(seg.wh,~temp)[[5]]
#scorescons[3,4]<-davies.test(seg.oy,~temp)[[5]]
#scorescons[4,4]<-davies.test(seg.bf,~temp)[[5]]
#scorescons[5,4]<-davies.test(seg.fb,~temp)[[5]]
#scorescons[6,4]<-davies.test(seg.sk,~temp)[[5]]
#scorescons[7,4]<-davies.test(seg.wp,~temp)[[5]]
#scorescons[8,4]<-davies.test(seg.hm,~temp)[[5]]
#slope1
scorescons[1,5]<-as.data.frame(slope(seg.gb))[1,1]
scorescons[2,5]<-as.data.frame(slope(seg.wh))[1,1]
scorescons[3,5]<-as.data.frame(slope(seg.oy))[1,1]
scorescons[4,5]<-as.data.frame(slope(seg.bf))[1,1]
scorescons[5,5]<-as.data.frame(slope(seg.fb))[1,1]
scorescons[6,5]<-as.data.frame(slope(seg.sk))[1,1]
scorescons[7,5]<-as.data.frame(slope(seg.wp))[1,1]
scorescons[8,5]<-as.data.frame(slope(seg.hm))[1,1]  
#slope1Ci  
scorescons[1,6]<-as.data.frame(slope(seg.gb))[1,1]-as.data.frame(slope(seg.gb))[1,4]
scorescons[2,6]<-as.data.frame(slope(seg.wh))[1,1]-as.data.frame(slope(seg.wh))[1,4]
scorescons[3,6]<-as.data.frame(slope(seg.oy))[1,1]-as.data.frame(slope(seg.oy))[1,4]
scorescons[4,6]<-as.data.frame(slope(seg.bf))[1,1]-as.data.frame(slope(seg.bf))[1,4]
scorescons[5,6]<-as.data.frame(slope(seg.fb))[1,1]-as.data.frame(slope(seg.fb))[1,4]
scorescons[6,6]<-as.data.frame(slope(seg.sk))[1,1]-as.data.frame(slope(seg.sk))[1,4]
scorescons[7,6]<-as.data.frame(slope(seg.wp))[1,1]-as.data.frame(slope(seg.wp))[1,4]
scorescons[8,6]<-as.data.frame(slope(seg.hm))[1,1]-as.data.frame(slope(seg.hm))[1,4]
#slope2
scorescons[1,7]<-as.data.frame(slope(seg.gb))[2,1]
scorescons[2,7]<-as.data.frame(slope(seg.wh))[2,1]
scorescons[3,7]<-as.data.frame(slope(seg.oy))[2,1]
scorescons[4,7]<-as.data.frame(slope(seg.bf))[2,1]
scorescons[5,7]<-as.data.frame(slope(seg.fb))[2,1]
scorescons[6,7]<-as.data.frame(slope(seg.sk))[2,1]
scorescons[7,7]<-as.data.frame(slope(seg.wp))[2,1]
scorescons[8,7]<-as.data.frame(slope(seg.hm))[2,1]
#slope2ci
scorescons[1,8]<-as.data.frame(slope(seg.gb))[2,1]-as.data.frame(slope(seg.gb))[2,4]
scorescons[2,8]<-as.data.frame(slope(seg.wh))[2,1]-as.data.frame(slope(seg.wh))[2,4]
scorescons[3,8]<-as.data.frame(slope(seg.oy))[2,1]-as.data.frame(slope(seg.oy))[2,4]
scorescons[4,8]<-as.data.frame(slope(seg.bf))[2,1]-as.data.frame(slope(seg.bf))[2,4]
scorescons[5,8]<-as.data.frame(slope(seg.fb))[2,1]-as.data.frame(slope(seg.fb))[2,4]
scorescons[6,8]<-as.data.frame(slope(seg.sk))[2,1]-as.data.frame(slope(seg.sk))[2,4]
scorescons[7,8]<-as.data.frame(slope(seg.wp))[2,1]-as.data.frame(slope(seg.wp))[2,4]
scorescons[8,8]<-as.data.frame(slope(seg.hm))[2,1]-as.data.frame(slope(seg.hm))[2,4]


scorescons

#write.csv(scorescons,"C:/Users/drewv/Documents/UMASS/data/cons_slopes.csv",row.names=F)
```

# Breakpoint analysis

## Breakpoint, y - Maximal trait performance

```{r,echo=F,warning=F}
   ###################################3
   #y point (trait performance)
  #brkpts_con<-filter(brkpts_con,pop!="Skidaway")
   
   mods.brk_con<-list(
  "nullo"=glm(brkpty~1,brkpts_con,family="gaussian"),
  "lat"=glm(brkpty~lat,brkpts_con,family="gaussian"),
  "mean"=glm(brkpty~mean,brkpts_con,family="gaussian"),
  "s.mean"=glm(brkpty~s.mean,brkpts_con,family="gaussian"),
  "q.mean"=glm(brkpty~q.mean,brkpts_con,family="gaussian"),
  "t.mean"=glm(brkpty~t.mean,brkpts_con,family="gaussian"),
  "seasonlength10"=glm(brkpty~seasonlength10,brkpts_con,family="gaussian"),
  "seasonlength16"=glm(brkpty~seasonlength16,brkpts_con,family="gaussian"),
  "omax"=glm(brkpty~max,brkpts_con,family="gaussian"),
  "spring"=glm(brkpty~spring,brkpts_con,family="gaussian"),
    "spring2"=glm(brkpty~spring2,brkpts_con,family="gaussian")
  )
   
   
aictab(mods.brk_con)
summary(lm(brkpty~mean,brkpts_con))
   
   ggplot(brkpts_con,aes(x=mean,y=brkpty))+
      geom_point(aes(color=oce),size=3)+theme_classic()+ylab(label="Maximal Consumption Rate \n log(oysters/day)")+scale_color_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))+
    scale_fill_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))+theme(text=element_text(family="arial",size=22))+xlab("MAT (°C)")+scale_x_continuous(limits=c(12,22),breaks=seq(12,22,by=2)) 
   
```

Here, we see that mean temperature is the best predictor according to AIC. Populations from warmer sites tend to have lower consumption rates, although this was not signficant (P>0.05). This is not evidence for countergradient variation in consumption rates. 

## Breakpoint, x - thermal optima


```{r,echo=F,warning=F}
     ###############################################3
   #x point (t opt)
   mods.brk_con<-list(
  "nullo"=glm(brkptx~1,brkpts_con,family="gaussian"),
  "lat"=glm(brkptx~lat,brkpts_con,family="gaussian"),
  "mean"=glm(brkptx~mean,brkpts_con,family="gaussian"),
  "s.mean"=glm(brkptx~s.mean,brkpts_con,family="gaussian"),
  "q.mean"=glm(brkptx~q.mean,brkpts_con,family="gaussian"),
  "t.mean"=glm(brkptx~t.mean,brkpts_con,family="gaussian"),
  "seasonlength10"=glm(brkptx~seasonlength10,brkpts_con,family="gaussian"),
  "seasonlength16"=glm(brkptx~seasonlength16,brkpts_con,family="gaussian"),
  "omax"=glm(brkptx~max,brkpts_con,family="gaussian"),
  "spring"=glm(brkptx~spring,brkpts_con,family="gaussian"),
    "spring2"=glm(brkptx~spring2,brkpts_con,family="gaussian")
  )
   
   aictab(mods.brk_con)
   summary(lm(brkptx~max,brkpts_con))
   
   ggplot(brkpts_con,aes(x=max,y=brkptx))+
      geom_point(aes(color=oce),size=3)+theme_classic()+ylab(label="Thermal Optima (°C)")+
      scale_color_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))+
      scale_fill_manual(labels=c("Atlantic","Pacific"),name="Ocean",
      values=c('red','blue'))+theme(text=element_text(family="arial",size=22))+
      xlab('Summer Maximum (°C)')+scale_x_continuous(limits=c(20,34),breaks=seq(20,34,by=2)) 
```

Here, we see mean site temperature again as the best predictor, and that the pattern of decreasing thermal optima with mean site temperature is preserved so that warmer sites perform best at lower temperatures. However, this pattern was not signficant (P>0.05)


## Quadratic

One reason we are checking quadratic is that sometimes the optima isn't the optima! If you closely examine the segmented regressions, you see that in some cases the second segment's slope does not appear to be negative. Instead, it plateaus. This raises the question whether we can really define our breakpoint as optima. Potentially, we could interpret the breakpoints as the lowest temperature of maximum weight At any rate, one idea is to redo all the analysis we just did for segmented regression but with a quadratic model, and seeing if we get a similar result in both the stacking but also the trends of the maximal weight and optima as we did with the segmented regression. 

```{r,echo=F,warning=F}
   
   #####################################################################################
   #################################################################################
   ##Quadratic
   
   quadm<-(glm(no.consumed~poly(uro.con3$Temp,2)+Population,uro.con3,family=poisson,offset=log(duration)))
   hist(resid(quadm))
   plot(quadm)
   
  
   bf<-filter(uro.con3,Population=="Beaufort")
   fb<-filter(uro.con3,Population=="Folly Beach")
   gb<-filter(uro.con3,Population=="Great Bay")
   hm<-filter(uro.con3,Population=="Humboldt")
   oy<-filter(uro.con3,Population=="Oyster")
   sk<-filter(uro.con3,Population=="Skidaway")
   wp<-filter(uro.con3,Population=="Willapa")
   wh<-filter(uro.con3,Population=="Woods Hole")
   
   xminq<-min(poly(uro.con3$Temp,2),na.rm=T)
   xmaxq<-max(poly(uro.con3$Temp,2),na.rm=T)
   
   qmgb<-(glm(no.consumed~poly(gb$Temp,2,raw=T),gb,family=poisson,offset=log(duration)))
   cfgb<-coef(qmgb)
   brkpts_con[3,11]<-(-cfgb[2]/(2*(cfgb[3])))
   brkpts_con[3,12]<-cfgb[1]+cfgb[2]*brkpts_con[3,11]+cfgb[3]*(brkpts_con[3,11]^2)
   
   qmwh<-(glm(no.consumed~poly(wh$Temp,2,raw=T),wh,family=poisson,offset=log(duration)))
   cfwh<-coef(qmwh)
   brkpts_con[4,11]<-(-cfwh[2]/(2*(cfwh[3])))
   brkpts_con[4,12]<-cfwh[1]+cfwh[2]*brkpts_con[4,11]+cfwh[3]*(brkpts_con[4,11]^2)
   
   qmoy<-(glm(no.consumed~poly(oy$Temp,2,raw=T),oy,family=poisson,offset=log(duration)))
   cfoy<-coef(qmoy)
   brkpts_con[5,11]<-(-cfoy[2]/(2*(cfoy[3])))
   brkpts_con[5,12]<-cfoy[1]+cfoy[2]*brkpts_con[5,11]+cfoy[3]*(brkpts_con[5,11]^2)
   
   qmbf<-(glm(no.consumed~poly(bf$Temp,2,raw=T),bf,family=poisson,offset=log(duration)))
   cfbf<-coef(qmbf)
   brkpts_con[6,11]<-(-cfbf[2]/(2*(cfbf[3])))
   brkpts_con[6,12]<-cfbf[1]+cfbf[2]*brkpts_con[6,11]+cfbf[3]*(brkpts_con[6,11]^2)
   
   
   qmfb<-(glm(no.consumed~poly(fb$Temp,2,raw=T),fb,family=poisson,offset=log(duration)))
   cffb<-coef(qmfb)
   brkpts_con[7,11]<-(-cffb[2]/(2*(cffb[3])))
   brkpts_con[7,12]<-cffb[1]+cffb[2]*brkpts_con[7,11]+cffb[3]*(brkpts_con[7,11]^2)
   
   qmsk<-(glm(no.consumed~poly(sk$Temp,2,raw=T),sk,family=poisson,offset=log(duration)))
   cfsk<-coef(qmsk)
   brkpts_con[8,11]<-(-cfsk[2]/(2*(cfsk[3])))
   brkpts_con[8,12]<-cfsk[1]+cfsk[2]*brkpts_con[8,11]+cfsk[3]*(brkpts_con[8,11]^2)
   
   qmwp<-(glm(no.consumed~poly(wp$Temp,2,raw=T),wp,family=poisson,offset=log(duration)))
   cfwp<-coef(qmwp)
   brkpts_con[1,11]<-(-cfwp[2]/(2*(cfwp[3])))
   brkpts_con[1,12]<-cfwp[1]+cfwp[2]*brkpts_con[1,11]+cfwp[3]*(brkpts_con[1,11]^2)
   
   qmhm<-(glm(no.consumed~poly(hm$Temp,2,raw=T),hm,family=poisson,offset=log(duration)))
   cfhm<-coef(qmhm)
   brkpts_con[2,11]<-(-cfhm[2]/(2*(cfhm[3])))
   brkpts_con[2,12]<-cfhm[1]+cfhm[2]*brkpts_con[2,11]+cfhm[3]*(brkpts_con[2,11]^2)
   
   
   uro.con3$Population<-factor(uro.con3$Population,levels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"))
   
   #poisson graph - not a good visual
  # ggplot(uro.con3,aes(y=no.consumed,x=Temp,group=Population,color=Population,linetype=Population))+
  #   scale_x_continuous(breaks=c(16,20,24,26,28,30))+
  #    scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+
  #    scale_linetype_manual(values=c(1,1,1,1,1,1,3,3),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+
  #    scale_size_manual(values=c(1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2),labels=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Skidaway","Willapa","Humboldt"),name="Population")+
  #    theme_classic()+labs(y="Rate of Consumption",x="Temperature (?C)")+facet_wrap(Population~.)+geom_point()
   
   uro.con3$rate<-log(uro.con3$rate)
   ggplot(uro.con3,aes(y=rate,x=Temp,group=Population,linetype=Population))+
      geom_smooth(method='glm',formula=y~poly(x,2),se=F,aes(color=Population,size=Population))+theme_classic()+
      scale_x_continuous(breaks=c(16,20,24,26,28,30))+
      scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      scale_linetype_manual(values=c(1,1,1,1,1,1,4,4),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      scale_size_manual(values=c(1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      theme_classic()+labs(y="Consumption rate (log(oysters/day))",x="Temperature (°C)")+theme(text=element_text(family="arial",size=22))
   
   
   
   ggplot(uro.con3,aes(y=rate,x=Temp,group=Population,linetype=Population))+
      geom_smooth(method='glm',formula=y~poly(x,2),se=F,aes(color=Population,size=Population))+theme_classic()+
      scale_x_continuous(breaks=c(16,20,24,26,28,30))+
      scale_color_manual(values=c("dark violet","navy","forest green","gold","dark orange","tomato","dark violet","navy"),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      scale_linetype_manual(values=c(1,1,1,1,1,1,3,3),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      scale_size_manual(values=c(1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2),labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Skidaway, GA","Willapa, WA","Humboldt, CA"),name="Population")+
      theme_classic()+labs(y="Log Rate of Consumption",x="Temperature (°C)")+facet_wrap(Population~.)+geom_point()+theme(legend.position = "none")+theme(text=element_text(family="arial",size=22))
```

## Y Breakpoint, maximal trait performance, quadratic

```{r,echo=F,warning=F}
   
   mods.brkyq<-list(
    "nullo"=glm(brkptyq~1,brkpts_con,family="gaussian"),
  "lat"=glm(brkptyq~lat,brkpts_con,family="gaussian"),
  "mean"=glm(brkptyq~mean,brkpts_con,family="gaussian"),
  "s.mean"=glm(brkptyq~s.mean,brkpts_con,family="gaussian"),
  "q.mean"=glm(brkptyq~q.mean,brkpts_con,family="gaussian"),
  "t.mean"=glm(brkptyq~t.mean,brkpts_con,family="gaussian"),
  "seasonlength10"=glm(brkptyq~seasonlength10,brkpts_con,family="gaussian"),
  "seasonlength16"=glm(brkptyq~seasonlength16,brkpts_con,family="gaussian"),
  "omax"=glm(brkptyq~max,brkpts_con,family="gaussian"),
  "spring"=glm(brkptyq~spring,brkpts_con,family="gaussian"),
  "spring2"=glm(brkptyq~spring2,brkpts_con,family="gaussian")

  )
   
   aictab(mods.brkyq)
   
   brkptxmq<-lm(brkptyq~max,brkpts_con)
   summary(brkptxmq)
   
   ggplot(brkpts_con,aes(x=lat,y=brkptyq))+
      geom_point(aes(color=oce),size=3)+geom_smooth(method="lm",se=F,color="black")+
      theme_classic()+ylab("Maximal Consumption (log(oysters/day))")+
      scale_color_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))+
      scale_fill_manual(labels=c("Atlantic","Pacific"),name="Ocean",
      values=c('red','blue'))+theme(text=element_text(family="arial",size=22))+
      xlab('Latitude')
   
```

Using quadratic, there is no significant relationship between environment (mean temp) and maximal trait performance. While this doesn't confirm that quadratic is a worse pick over segmented, our segmented results have lower p values and confirms what we've seen with growth and length, that segmented works best. 

## X breakpoint, thermal optima, quadratic

```{r,echo=F,warning=F}
   
   mods.brkxq<-list(
   "nullo"=glm(brkptxq~1,brkpts_con,family="gaussian"),
  "lat"=glm(brkptxq~lat,brkpts_con,family="gaussian"),
  "mean"=glm(brkptxq~mean,brkpts_con,family="gaussian"),
  "s.mean"=glm(brkptxq~s.mean,brkpts_con,family="gaussian"),
  "q.mean"=glm(brkptxq~q.mean,brkpts_con,family="gaussian"),
  "t.mean"=glm(brkptxq~t.mean,brkpts_con,family="gaussian"),
  "seasonlength10"=glm(brkptxq~seasonlength10,brkpts_con,family="gaussian"),
  "seasonlength16"=glm(brkptxq~seasonlength16,brkpts_con,family="gaussian"),
  "omax"=glm(brkptxq~max,brkpts_con,family="gaussian"),
  "spring"=glm(brkptxq~spring,brkpts_con,family="gaussian"),
  "spring1"=glm(brkptxq~spring2,brkpts_con,family="gaussian")
  )
 
   
   aictab(mods.brkxq)
   
   brkptxmq<-lm(brkptxq~spring2,brkpts_con)
   summary(brkptxmq)
   
   ggplot(brkpts_con,aes(x=seasonlength10,y=brkptxq))+
      geom_point(size=3,aes(color=oce))+geom_smooth(method="lm",se=F,color="black")+theme_classic()+
      ylab("Thermal Optima")+
      xlab("Season Length above 10C")+
      theme(text=element_text(family="arial",size=22))+
      scale_color_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))+
      scale_fill_manual(labels=c("Atlantic","Pacific"),name="Ocean", values=c('red','blue'))
```

Finally, let us compare the breakpoints we have extracted from segmented and quadratic regression. We want to know if we see significantly different results in methods, because if we do then we have to be careful about which once we select (seg v. quad). We see here that while thermal optima estimates differ significantly between quad and seg, they do not when estimating maximal trait performance. Importantly, the sign of both relationships is the same. Therefore, we can be confident that our conclusion of countergradient variation in maximal trait performance is correct.
```{r,echo=F,warning=F}
brkptnewx<-gather(brkpts_con, modeltypex, x, c(brkptx,brkptxq)) 
brkptnewy<-gather(brkpts_con,modeltypey,y,c(brkpty,brkptyq))

ggplot(brkptnewx,aes(x=seasonlength10,y=x,group=modeltypex,color=modeltypex))+geom_point()+theme_classic()+scale_color_manual(labels=c("Segmented","Quadratic"),name="Thermal Optima",values=c("red","blue"))+geom_smooth(method=lm,se=F)

ggplot(brkptnewy,aes(x=seasonlength10,y=y,group=modeltypey,color=modeltypey))+geom_point()+theme_classic()+scale_color_manual(labels=c("Segmented","Quadratic"),name="Maximal Trait Performance",values=c("red","blue"))+geom_smooth(method=lm,se=F)

brkptnewx$pop<-as.character(brkptnewx$pop)
summary(lm(x~modeltypex,brkptnewx))

brkptnewy$pop<-as.character(brkptnewy$pop)
summary(lm(y~modeltypey,brkptnewy))
```


```{r,include=F}

#write.csv(brkpts_con,"C:/Users/drewv/Documents/UMASS/data/cons_brkpts.csv",row.names=F)
```